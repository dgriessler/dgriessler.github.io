<!DOCTYPE html>

<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
     <meta charset="utf-8" />
     <title>Improving Calibration for Long-Tailed Recognition by Zhisheng Zhong et al.</title>
</head>
<body>
     <h1>Reference</h1>
     <h2>Title</h2>
     <p>Improving Calibration for Long-Tailed Recognition</p>
     <h2>Authors</h2>
     <p>Zhisheng Zhong</p>
     <p>Jiequan Cui</p>
     <p>Shu Liu</p>
     <p>Jiaya Jia</p>
     <p>Chinese University of Hong Kong</p>
     <p>SmartMore</p>
     <h2>Year</h2>
     <p>2022</p>
     <h2>Venue</h2>
     <p>CVPR</p>
     <h2>Paper Numbers</h2>
     <p>1-15</p>
     <h1>Paper Link</h1>
     <a href="https://arxiv.org/pdf/2104.00466.pdf">Improving Calibration for Long-Tailed Recognition</a>
     <h1>Summary</h1>
     <p>
          This paper references mixup which is detailed in the paper <a href="https://arxiv.org/abs/1710.09412">mixup: Beyond Empirical Risk Minimization</a>.
          The idea is to combine two examples together using a delta term ~ Beta(alpha, alpha). Their approach introduces the idea of label-aware smoothing
          to solve over-confidence from the normal method with the goal of penalizing classes with larger instance numbers. They propose several ways of
          computing the function based on the instance number of a class such concave, linear, and convex forms. They find their method performs better than
          CB-CE in terms of test accuracy. They vary and find optimal values for the two hyperparameters e1 and eK used in the function for label aware smoothing. 
     </p>
</body>
</html>
