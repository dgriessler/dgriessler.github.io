<!DOCTYPE html>

<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
     <meta charset="utf-8" />
     <title>Distribution Alignment: A Unified Framework for Long-tail Visual Recognition by Songyang Zhang et al.</title>
</head>
<body>
     <h1>Reference</h1>
     <h2>Title</h2>
     <p>Distribution Alignment: A Unified Framework for Long-tail Visual Recognition</p>
     <h2>Authors</h2>
     <p>Songyang Zhang</p>
     <p>Zeming Li</p>
     <p>Shipeng Yan</p>
     <p>Xuming He</p>
     <p>Jian Sun</p>
     <p>ShanghaiTech University</p>
     <p>Megvii Technology</p>
     <p>University of Chinese Academy of Sciences</p>
     <p>Shanghai Engineering Research Center of Intelligent Vision and Imaging</p>
     <p>Shanghai Institute of Microssytem and Information Technology, Chinese CAcademy of Sciences</p>
     <h2>Year</h2>
     <p>2021</p>
     <h2>Venue</h2>
     <p>CVPR</p>
     <h2>Paper Numbers</h2>
     <p>1-15</p>
     <h1>Paper Link</h1>
     <a href="https://arxiv.org/pdf/2103.16370.pdf">Distribution Alignment: A Unified Framework for Long-tail Visual Recognition</a>
     <h1>Summary</h1>
     <p>
          This paper analyzes the performance bottleneck in the classic two-stage learning strategy. They find that the representation
          is learned well from the unbalanced data, but that the second stage can be improved due to the biased decision boundary. They outline
          two stages: joint learning stage, the normal NN training on the inbalanced dataset, and distribution calibration stage, freeze the 
          representation and calibrate the class scores using an adaptive calibration function and a distruction alignment strategy with
          generalized reweighting. The Adaptive Calibration Function is outlined in detail to get to the class score, the probability a class
          is predicted given some input. The Alignment with Generalized Re-weighting uses the KL-divergence between these class scores and the
          expected ones. However, for the KL-divergence to be used it has to be based on probability distribution which should sum to 1. Their
          outline of the weighted empirical distribution does not sum to 1 making their method suspect. Nevertheless, they achieve good results.
     </p>
</body>
</html>
