<!DOCTYPE html>

<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
     <meta charset="utf-8" />
     <title>Forecasting SEP Events based on Merged CME Catalogs using Machine Learning by Peter William Tarsoly</title>
</head>
<body>
     <h1>Reference</h1>
     <h2>Title</h2>
     <p>Forecasting SEP Events based on Merged CME Catalogs using Machine Learning</p>
     <h2>Authors</h2>
     <p>Peter William Tarsoly</p>
     <h2>Year</h2>
     <p>2021</p>
     <h2>Venue</h2>
     <p>Florida Tech</p>
     <h2>Paper Numbers</h2>
     <p>1-73</p>
     <h1>Paper Link</h1>
     <a href="https://cs.fit.edu/~pkc/theses/tarsoly21.pdf">Forecasting SEP Events based on Merged CME Catalogs using Machine Learning</a>
     <h1>Summary</h1>
     <p>An SEP event occurs when the flux of >10 MeV protons is greater than 10 proton flux units (pfus). SEPs are driven by CMEs (Coronal Mass Ejections.
     The problem addressed is to forecase the occurence of SEP events using CME measurements. The DONKI dataset is combined with the CDAW CME dataset. 
     CDAW measurements are earth based while DONKI measurements are Suncentric instrument based. There are some challenges to combining the sets like 
     the limited 7 year span of the DONKI set and duplicate entries can exist between the datasets. Matches are made by resolving conflicts with many CDAW
     candidates to one DONKI, multiple DONKI entires to one CDAW, no CDAW, and a one-to-one match. Duplicates are also removed and the time span is limited
     to include alignments between the two sets. The final set contains 2,394 CMEs from the original 2585 from DONKI and 29217 from CDAW. Only 39 are associated with SEPs.</p>
     <p>The authors also presented a NN to predict whether a CME may be related to an upcoming SEP event thus a classification problem if a CME is SEP or non-SEP related. Improvements are made
     to Torres work by increasing the quality of data and utilizing additional ML techniques. The enhanced data comes from the CDAW/DONKI combo dataset produced in the last paragraph. Feature 
     and representation learning are included as addiitonal ML techniques. The NN is a multi-layer perceptron NN using one hidden layer like Torres. Unlike Torres, Tarsoly uses 2 output units 
     semantically equivalent to Torres but allows for additional techniques to improve performance. 3 groups of features are made including DONKI features, selected features from derived and external
     data, and remaining CDAW features. Data-based imbalanced data is handled via oversampling. Model-based imbalanced data was hanlded using a 2 stage training via classifier re-training (cRT). The
     first stage is to learn better features from the hidden units and the second stage is to freeze the hidden and input layers and relearn the hidden to output weights. The second stage is then focused
     on learning the classifer. Better features were learned using an autoencoder taking input to some new representation and then back again.</p>
     <p>The dataset was partitioned chronically and randomly for two different schemes. A confusion matrix was used again for evaluation including calcuations of precision, recall, F1, and TSS along with
     a new factor of HSS, Heidke Skill Score, to determine if the classifier was better than random. In addition, LPR, lowest point average rank, and HNR, highest negative average rank, were computed.
     Sigmoid units were used in all units. There were only 3 hidden units instead of 30 like Torres. Momentum was set to 0.9 and L2 weight regularization to 0.0075. The model was trained over 400 epochs for
     the first stage and then again for the second stage with frozen weights between input to hidden and retrained weights from hidden to output.</p>
     <p>TSS, HSS, and F1 improve with more features. Although cRT and cRT + AE both increase TSS, HSS, and F1, they also vary in rank improvement over original single-stage model. The output threshold was
     varied with the best results using cRT + AE for chronological and less clear results for the random partioning. Large and fast CMEs are predicted better in the baseline verison. Feature importance was 
     calcuated using LIME and identify 5 main groups Speed, Location, Size, CME History, and Others for the input parameters. The speed and location are ranked highest. False negatives occured close to
     a lat/long of 0 and prolonged elevated proton flux in random partitioning. In chronological partitioning, false positives occur with high speed and large half width. False negatives are found when
     flux barely crosses the threshold, during an ESP event, and during a double CME.</p>
</body>
</html>
